{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4f4JG1gdKqj"
   },
   "source": [
    "# AutoEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jbiqOK7dLGG"
   },
   "source": [
    "# About the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XL5MEkLcfRD2"
   },
   "source": [
    "http://files.grouplens.org/datasets/movielens/ml-100k-README.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "rjOPzue7FCXJ",
    "outputId": "b0f0b631-51bf-49e8-9ad6-947a0bd3d832"
   },
   "source": [
    "http://files.grouplens.org/datasets/movielens/ml-1m.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Xis6ldDfTs6"
   },
   "source": [
    "http://files.grouplens.org/datasets/movielens/ml-100k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "LOly1yfAfTjd",
    "outputId": "519b7fb3-4f15-4a0b-91d5-8221ee486409"
   },
   "source": [
    "The purpose of this AutoEncoder is to learn latent representations of users and movies such that the model can predict missing ratings and provide personalized movie recommendations based on user preferences. The training and testing processes aim to optimize the model's ability to reconstruct the input ratings while generalizing to unseen data during testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOBJ8UCXdY0g"
   },
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LvGeU1CeCtg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pM04FyMudkoK"
   },
   "source": [
    "## Importing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJw2p3-Cewo4"
   },
   "outputs": [],
   "source": [
    "# We won't be using this dataset.\n",
    "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTIbE2tkdkwP"
   },
   "source": [
    "## Preparing the training set and the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2usLKJBEgPE2"
   },
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t')\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t')\n",
    "test_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zCf8HjSydk4s"
   },
   "source": [
    "## Getting the number of users and movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gPaGZqdniC5m"
   },
   "outputs": [],
   "source": [
    "nb_users = int(max(max(training_set[:, 0], ), max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1], ), max(test_set[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J-w4-hVidlAm"
   },
   "source": [
    "## Converting the data into an array with users in lines and movies in columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wASs2YFiDaa"
   },
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "  new_data = []\n",
    "  for id_users in range(1, nb_users + 1):\n",
    "    id_movies = data[:, 1] [data[:, 0] == id_users]\n",
    "    id_ratings = data[:, 2] [data[:, 0] == id_users]\n",
    "    ratings = np.zeros(nb_movies)\n",
    "    ratings[id_movies - 1] = id_ratings\n",
    "    new_data.append(list(ratings))\n",
    "  return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMmhuUpldlHo"
   },
   "source": [
    "## Converting the data into Torch tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwD-KD8yiEEw"
   },
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Differentiation: PyTorch tensors support automatic differentiation using the computation graph. This feature allows gradients to be computed automatically during backpropagation. It is essential for training neural networks using gradient-based optimization algorithms like stochastic gradient descent (SGD).\n",
    "\n",
    "GPU Acceleration: PyTorch tensors can easily be moved to and processed on GPUs, which accelerates the computations and significantly speeds up training for large models and datasets. NumPy arrays do not have built-in support for GPU acceleration.\n",
    "\n",
    "Seamless Integration with Neural Network Modules: PyTorch tensors seamlessly integrate with the PyTorch neural network modules (nn.Module). This integration allows users to define and train neural networks in a modular and organized manner.\n",
    "\n",
    "Efficient Operations: PyTorch provides optimized tensor operations using low-level libraries such as Intel MKL and NVIDIA cuBLAS. These operations are often faster than equivalent NumPy operations.\n",
    "\n",
    "Interoperability: PyTorch tensors can be converted to and from NumPy arrays easily using .numpy() and torch.from_numpy() functions. This interoperability allows users to leverage the strengths of both libraries when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6kkL8NkkdlZj"
   },
   "source": [
    "## Creating the architecture of the Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoEncoder aims to reconstruct the input data as accurately as possible, which helps in learning meaningful latent representations of users and movies for collaborative filtering and recommendation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oU2nyh76iE6M"
   },
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    #using nn.Module we can leverage the powers of PyTorch for building neural network models\n",
    "    def __init__(self, ):\n",
    "        # ARCHITECTURE OF AUTO ENCODER\n",
    "        super(SAE, self).__init__()\n",
    "        #call the constructor of nn.Module\n",
    "        self.fc1 = nn.Linear(nb_movies, 20) # first fully connected Dense layer of auto encoder nb_movies as input and \n",
    "        # produces 20 neurons\n",
    "        self.fc2 = nn.Linear(20, 10) # dense layer takes 20 as input , op of prev and produces 10 {dense layers}\n",
    "        self.fc3 = nn.Linear(10, 20) # dense layer\n",
    "        self.fc4 = nn.Linear(20, nb_movies) # dense layer that produces number of movies output neurons\n",
    "        self.activation = nn.Sigmoid() # activation function of this model is sigmoid\n",
    "    def forward(self, x):\n",
    "        #This method defines the forward pass of the AutoEncoder. \n",
    "        #It specifies how the input data x flows through the layers to produce the output.\n",
    "        \n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7gy59alAdloL"
   },
   "source": [
    "## Training the SAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FEz9hRaciFTs",
    "outputId": "0f6ed0d0-09c4-46c0-bfe6-70031d76b491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1loss: tensor(1.7713)\n",
      "epoch: 2loss: tensor(1.0967)\n",
      "epoch: 3loss: tensor(1.0533)\n",
      "epoch: 4loss: tensor(1.0383)\n",
      "epoch: 5loss: tensor(1.0307)\n",
      "epoch: 6loss: tensor(1.0269)\n",
      "epoch: 7loss: tensor(1.0239)\n",
      "epoch: 8loss: tensor(1.0220)\n",
      "epoch: 9loss: tensor(1.0207)\n",
      "epoch: 10loss: tensor(1.0199)\n",
      "epoch: 11loss: tensor(1.0191)\n",
      "epoch: 12loss: tensor(1.0185)\n",
      "epoch: 13loss: tensor(1.0181)\n",
      "epoch: 14loss: tensor(1.0176)\n",
      "epoch: 15loss: tensor(1.0174)\n",
      "epoch: 16loss: tensor(1.0167)\n",
      "epoch: 17loss: tensor(1.0168)\n",
      "epoch: 18loss: tensor(1.0165)\n",
      "epoch: 19loss: tensor(1.0164)\n",
      "epoch: 20loss: tensor(1.0162)\n",
      "epoch: 21loss: tensor(1.0161)\n",
      "epoch: 22loss: tensor(1.0163)\n",
      "epoch: 23loss: tensor(1.0158)\n",
      "epoch: 24loss: tensor(1.0158)\n",
      "epoch: 25loss: tensor(1.0158)\n",
      "epoch: 26loss: tensor(1.0159)\n",
      "epoch: 27loss: tensor(1.0154)\n",
      "epoch: 28loss: tensor(1.0148)\n",
      "epoch: 29loss: tensor(1.0127)\n",
      "epoch: 30loss: tensor(1.0112)\n",
      "epoch: 31loss: tensor(1.0100)\n",
      "epoch: 32loss: tensor(1.0067)\n",
      "epoch: 33loss: tensor(1.0068)\n",
      "epoch: 34loss: tensor(1.0034)\n",
      "epoch: 35loss: tensor(1.0027)\n",
      "epoch: 36loss: tensor(0.9990)\n",
      "epoch: 37loss: tensor(0.9983)\n",
      "epoch: 38loss: tensor(0.9959)\n",
      "epoch: 39loss: tensor(0.9935)\n",
      "epoch: 40loss: tensor(0.9902)\n",
      "epoch: 41loss: tensor(0.9903)\n",
      "epoch: 42loss: tensor(0.9911)\n",
      "epoch: 43loss: tensor(0.9894)\n",
      "epoch: 44loss: tensor(0.9864)\n",
      "epoch: 45loss: tensor(0.9890)\n",
      "epoch: 46loss: tensor(0.9862)\n",
      "epoch: 47loss: tensor(0.9905)\n",
      "epoch: 48loss: tensor(0.9849)\n",
      "epoch: 49loss: tensor(0.9832)\n",
      "epoch: 50loss: tensor(0.9794)\n",
      "epoch: 51loss: tensor(0.9823)\n",
      "epoch: 52loss: tensor(0.9859)\n",
      "epoch: 53loss: tensor(0.9886)\n",
      "epoch: 54loss: tensor(0.9822)\n",
      "epoch: 55loss: tensor(0.9819)\n",
      "epoch: 56loss: tensor(0.9785)\n",
      "epoch: 57loss: tensor(0.9747)\n",
      "epoch: 58loss: tensor(0.9711)\n",
      "epoch: 59loss: tensor(0.9702)\n",
      "epoch: 60loss: tensor(0.9649)\n",
      "epoch: 61loss: tensor(0.9674)\n",
      "epoch: 62loss: tensor(0.9646)\n",
      "epoch: 63loss: tensor(0.9633)\n",
      "epoch: 64loss: tensor(0.9660)\n",
      "epoch: 65loss: tensor(0.9655)\n",
      "epoch: 66loss: tensor(0.9646)\n",
      "epoch: 67loss: tensor(0.9644)\n",
      "epoch: 68loss: tensor(0.9582)\n",
      "epoch: 69loss: tensor(0.9577)\n",
      "epoch: 70loss: tensor(0.9555)\n",
      "epoch: 71loss: tensor(0.9622)\n",
      "epoch: 72loss: tensor(0.9591)\n",
      "epoch: 73loss: tensor(0.9562)\n",
      "epoch: 74loss: tensor(0.9551)\n",
      "epoch: 75loss: tensor(0.9558)\n",
      "epoch: 76loss: tensor(0.9552)\n",
      "epoch: 77loss: tensor(0.9565)\n",
      "epoch: 78loss: tensor(0.9545)\n",
      "epoch: 79loss: tensor(0.9509)\n",
      "epoch: 80loss: tensor(0.9510)\n",
      "epoch: 81loss: tensor(0.9495)\n",
      "epoch: 82loss: tensor(0.9478)\n",
      "epoch: 83loss: tensor(0.9496)\n",
      "epoch: 84loss: tensor(0.9472)\n",
      "epoch: 85loss: tensor(0.9459)\n",
      "epoch: 86loss: tensor(0.9449)\n",
      "epoch: 87loss: tensor(0.9441)\n",
      "epoch: 88loss: tensor(0.9432)\n",
      "epoch: 89loss: tensor(0.9428)\n",
      "epoch: 90loss: tensor(0.9420)\n",
      "epoch: 91loss: tensor(0.9414)\n",
      "epoch: 92loss: tensor(0.9394)\n",
      "epoch: 93loss: tensor(0.9430)\n",
      "epoch: 94loss: tensor(0.9379)\n",
      "epoch: 95loss: tensor(0.9394)\n",
      "epoch: 96loss: tensor(0.9375)\n",
      "epoch: 97loss: tensor(0.9384)\n",
      "epoch: 98loss: tensor(0.9374)\n",
      "epoch: 99loss: tensor(0.9379)\n",
      "epoch: 100loss: tensor(0.9362)\n",
      "epoch: 101loss: tensor(0.9365)\n",
      "epoch: 102loss: tensor(0.9350)\n",
      "epoch: 103loss: tensor(0.9363)\n",
      "epoch: 104loss: tensor(0.9390)\n",
      "epoch: 105loss: tensor(0.9337)\n",
      "epoch: 106loss: tensor(0.9320)\n",
      "epoch: 107loss: tensor(0.9327)\n",
      "epoch: 108loss: tensor(0.9322)\n",
      "epoch: 109loss: tensor(0.9336)\n",
      "epoch: 110loss: tensor(0.9317)\n",
      "epoch: 111loss: tensor(0.9323)\n",
      "epoch: 112loss: tensor(0.9318)\n",
      "epoch: 113loss: tensor(0.9314)\n",
      "epoch: 114loss: tensor(0.9309)\n",
      "epoch: 115loss: tensor(0.9309)\n",
      "epoch: 116loss: tensor(0.9292)\n",
      "epoch: 117loss: tensor(0.9303)\n",
      "epoch: 118loss: tensor(0.9299)\n",
      "epoch: 119loss: tensor(0.9296)\n",
      "epoch: 120loss: tensor(0.9290)\n",
      "epoch: 121loss: tensor(0.9294)\n",
      "epoch: 122loss: tensor(0.9290)\n",
      "epoch: 123loss: tensor(0.9284)\n",
      "epoch: 124loss: tensor(0.9278)\n",
      "epoch: 125loss: tensor(0.9279)\n",
      "epoch: 126loss: tensor(0.9272)\n",
      "epoch: 127loss: tensor(0.9274)\n",
      "epoch: 128loss: tensor(0.9265)\n",
      "epoch: 129loss: tensor(0.9267)\n",
      "epoch: 130loss: tensor(0.9262)\n",
      "epoch: 131loss: tensor(0.9260)\n",
      "epoch: 132loss: tensor(0.9255)\n",
      "epoch: 133loss: tensor(0.9254)\n",
      "epoch: 134loss: tensor(0.9251)\n",
      "epoch: 135loss: tensor(0.9251)\n",
      "epoch: 136loss: tensor(0.9246)\n",
      "epoch: 137loss: tensor(0.9246)\n",
      "epoch: 138loss: tensor(0.9241)\n",
      "epoch: 139loss: tensor(0.9239)\n",
      "epoch: 140loss: tensor(0.9235)\n",
      "epoch: 141loss: tensor(0.9237)\n",
      "epoch: 142loss: tensor(0.9231)\n",
      "epoch: 143loss: tensor(0.9231)\n",
      "epoch: 144loss: tensor(0.9225)\n",
      "epoch: 145loss: tensor(0.9226)\n",
      "epoch: 146loss: tensor(0.9222)\n",
      "epoch: 147loss: tensor(0.9221)\n",
      "epoch: 148loss: tensor(0.9217)\n",
      "epoch: 149loss: tensor(0.9218)\n",
      "epoch: 150loss: tensor(0.9212)\n",
      "epoch: 151loss: tensor(0.9213)\n",
      "epoch: 152loss: tensor(0.9208)\n",
      "epoch: 153loss: tensor(0.9208)\n",
      "epoch: 154loss: tensor(0.9203)\n",
      "epoch: 155loss: tensor(0.9200)\n",
      "epoch: 156loss: tensor(0.9196)\n",
      "epoch: 157loss: tensor(0.9196)\n",
      "epoch: 158loss: tensor(0.9195)\n",
      "epoch: 159loss: tensor(0.9196)\n",
      "epoch: 160loss: tensor(0.9192)\n",
      "epoch: 161loss: tensor(0.9189)\n",
      "epoch: 162loss: tensor(0.9185)\n",
      "epoch: 163loss: tensor(0.9186)\n",
      "epoch: 164loss: tensor(0.9184)\n",
      "epoch: 165loss: tensor(0.9181)\n",
      "epoch: 166loss: tensor(0.9179)\n",
      "epoch: 167loss: tensor(0.9179)\n",
      "epoch: 168loss: tensor(0.9178)\n",
      "epoch: 169loss: tensor(0.9176)\n",
      "epoch: 170loss: tensor(0.9174)\n",
      "epoch: 171loss: tensor(0.9174)\n",
      "epoch: 172loss: tensor(0.9173)\n",
      "epoch: 173loss: tensor(0.9171)\n",
      "epoch: 174loss: tensor(0.9168)\n",
      "epoch: 175loss: tensor(0.9169)\n",
      "epoch: 176loss: tensor(0.9163)\n",
      "epoch: 177loss: tensor(0.9168)\n",
      "epoch: 178loss: tensor(0.9166)\n",
      "epoch: 179loss: tensor(0.9164)\n",
      "epoch: 180loss: tensor(0.9160)\n",
      "epoch: 181loss: tensor(0.9160)\n",
      "epoch: 182loss: tensor(0.9157)\n",
      "epoch: 183loss: tensor(0.9160)\n",
      "epoch: 184loss: tensor(0.9154)\n",
      "epoch: 185loss: tensor(0.9159)\n",
      "epoch: 186loss: tensor(0.9149)\n",
      "epoch: 187loss: tensor(0.9154)\n",
      "epoch: 188loss: tensor(0.9150)\n",
      "epoch: 189loss: tensor(0.9153)\n",
      "epoch: 190loss: tensor(0.9146)\n",
      "epoch: 191loss: tensor(0.9147)\n",
      "epoch: 192loss: tensor(0.9142)\n",
      "epoch: 193loss: tensor(0.9144)\n",
      "epoch: 194loss: tensor(0.9140)\n",
      "epoch: 195loss: tensor(0.9143)\n",
      "epoch: 196loss: tensor(0.9138)\n",
      "epoch: 197loss: tensor(0.9140)\n",
      "epoch: 198loss: tensor(0.9140)\n",
      "epoch: 199loss: tensor(0.9136)\n",
      "epoch: 200loss: tensor(0.9136)\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "  train_loss = 0\n",
    "  s = 0.\n",
    "  for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = input.clone()\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "      output = sae(input)\n",
    "      target.require_grad = False\n",
    "      output[target == 0] = 0\n",
    "      loss = criterion(output, target)\n",
    "      mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "      loss.backward()\n",
    "      train_loss += np.sqrt(loss.data*mean_corrector)\n",
    "      s += 1.\n",
    "      optimizer.step()\n",
    "  print('epoch: '+str(epoch)+'loss: '+ str(train_loss/s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bak5uc8gd-gX"
   },
   "source": [
    "## Testing the SAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5ztvzYRtiGCz",
    "outputId": "d0e8ea8b-9ac4-40e5-a19a-7fcfc6934d61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: tensor(0.9484)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "  input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "  target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "  if torch.sum(target.data > 0) > 0:\n",
    "    output = sae(input)\n",
    "    target.require_grad = False\n",
    "    output[target == 0] = 0\n",
    "    loss = criterion(output, target)\n",
    "    mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "    test_loss += np.sqrt(loss.data*mean_corrector)\n",
    "    s += 1.\n",
    "print('test loss: '+str(test_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AutoEncoders.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
